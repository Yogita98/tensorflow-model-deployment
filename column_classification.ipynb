{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "column_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yogita98/tensorflow-model-deployment/blob/master/column_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyM94owUpNz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7b51457f-f543-4e26-d349-f0bde10bc59d"
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTxU9jN-yisA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cat /var/log/colab-jupyter.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dbf4GGLpbgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RWSJpsyKqHjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf96a1db-e008-4635-b526-b5e0e146088b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7cBHyzGpbpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATADIR = \"\"\n",
        "\n",
        "CATEGORIES = [\"label1\", \"label2\", \"labeln\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBrXj7jwpbzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for category in CATEGORIES:  \n",
        "    path = os.path.join(DATADIR,category)  \n",
        "    for img in os.listdir(path):  # iterate over each image per label\n",
        "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "        plt.imshow(img_array, cmap='gray')  # graph it\n",
        "        plt.show()  # display!\n",
        "\n",
        "        break  # we just want one for now so break\n",
        "     break  #...and one more!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC5O0bRDpmk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXp5cLTXtiqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(img_array.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS8J7Qikti0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "012aa7dc-78c6-4411-9c88-34f6a0edbe8e"
      },
      "source": [
        "IMG_SIZE = 150\n",
        "\n",
        "new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n",
        "plt.imshow(new_array, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAcp0lEQVR4nO3df5Ac5Z3f8fd3ZnZ2tT8krRZpkVZarcCSbCFwRC0KLg5iH7ER4LMuFRfG5cr57qiiUvEldnKpCz7+yP3hqzrnEjt3VWdT5HDsS3FwPs6UsWPHJsRgp8pSvEICIRksgUEsXokVSCtpf82vb/6Y7nHvaPaHdmZ2Zrc/r6qpnenp6X62Z55vP/109/M1d0dE4ivR6AKISGMpCIjEnIKASMwpCIjEnIKASMwpCIjEXN2CgJntM7NXzOykmT1Qr/WISHWsHtcJmFkS+AXwYWAY+BnwSXc/XvOViUhV6tUS2AucdPfX3D0DPA7sr9O6RKQKqTottw94M/J6GPjHs8181VVX+cDAQJ2KIiIAhw4dOuvu68un1ysIzMvM7gfuB+jv72doaKhRRRGJBTN7o9L0eh0OvAVsibzeHEwrcfeH3X3Q3QfXr78sOInIEqlXEPgZsN3MtplZGrgXeKpO6xKRKtTlcMDdc2b2B8APgCTwNXc/Vo91iUh16tYn4O7fA75Xr+WLSG3oikGRmFMQEIk5BQGRmGvYdQLlNMyZyPzMrObLbJogIM2l2h+bgnp91GO7Nk0QqEeEi7NqfyzuXnEZlb6nub47fa+1taKDgDSXQqFAPp+nUCjg7uTzeQASicSMh5nNqOiq9PWlwwFZsPl+LOEeJbpnCff+YcVuaWnhkUceYWhoiIMHD5LNZhkYGOD666/ntttuY+fOnfT19QHMCArRZSooND8FgZiarVkfBoHQ5OQkY2NjjI6OMj4+zqpVq+jv7yebzeLuJJNJgIqtAlkeFARirrzSRl8XCoUZFTufz5f28olEgmQySTKZnHMZ0vwUBKSk0qFBLpcjm80yPT3N+Pg42WyWRCJBKpWiUCiQyWRIJpOllgBQ6j8ws1JLQZqXgoBcJgwA7s6tt97Ktddey+2338709DQ9PT1s3ryZbdu2sXbt2ssCABRbCbJ8KAjIrMyMG264gRtuuKH02swuq+Q6HFjeFASkJKy80Uqs5vzKp3abSMwpCIjEnIKASMwpCIjEnIKASMwpCIjE3KKDgJltMbMfmdlxMztmZp8Npq8zs6fN7ETwt7t2xRWRWqumJZAD/tDddwE3A58xs13AA8Az7r4deCZ4LSJNatFBwN1H3P354PlF4OcUcxDuB74RzPYN4LerLaSI1E9N+gTMbADYAxwEet19JHjrNNBbi3WISH1UHQTMrBP4B+Bz7n4h+p4Xb0urOB6Smd1vZkNmNjQ6OlptMURkkaoKAmbWQjEAPOru3womnzGzjcH7G4G3K31WCUlFmkM1ZwcMeAT4ubt/KfLWU8Cng+efBr69+OKJSL1VcxfhLcC/AI6a2ZFg2h8DfwZ808zuA94A7qmuiCJST4sOAu7+f4HZbhy/fbHLFZGlpSsGRWJOQUAk5ppiZKFMJsPw8HCjiyESS00RBEZHR/nKV77S6GKIxJI1Q+LIG264wb/73e82uhgiK9rWrVsPuftg+fSmaAmk02n6+/sbXQyRWFLHoEjMKQiIxJyCgEjMKQiIxJyCgEjMKQiIxJyCgEjMNcV1AlBMhy0is6tXHWmaIKB01iJzq1cd0eGASMwpCIjEnIKASMwpCIjEnIKASMzVIvlI0swOm9l3g9fbzOygmZ00s78zs3T1xRSReqlFS+CzFPMQhr4IfNnd3wOcA+6rwTpEpE6qzUC0Gbgb+OvgtQG/CTwRzKKEpCJNrtqLhf4r8EdAV/C6Bzjv7rng9TDFTMUiUiV3Lz2iFw5VexHRooOAmX0UeNvdD5nZBxfx+fuB+wH6+/t12bDIAs1X6SvVpbnqV7VpyD5mZncBbcBq4C+AtWaWCloDm4G3Zinow8DDAIODg4oAIguwkL2+mZUqffnfShbdJ+Dun3f3ze4+ANwL/B93/xTwI+DjwWxKSLpI0abflT6g+EOo9lELzVKOlWKh2yyRSJBIJEgmkySTSVKp2ff39biB6D8Aj5vZF4DDFDMXz0tfdm1F9wDhti3fxhcuXODcuXO8+uqrvP7667z77rtkMhk2bNjAli1buOWWW1i1ahWJRGJGgHF3zp8/z+joKKOjo7z00kuMj4+Tz+fp7Ozklltuoa+vj56eHhKJxGVlqlSWub5//TbqqyZBwN2fBZ4Nnr8G7K3FcuOs2h9+tMK6e6kiR124cIFTp07x3HPP8eMf/5iTJ08yPj7Orl27+MAHPsCePXtobW0tNS/z+TxmRqFQ4Ny5c5w4cYJjx47xxBNPcPr0aTKZDFdffTVdXV20trbS3d1d+l/K113euVXL/12uTNPcSiy1FVakZDI5Y3p573J5kzu6x08kEqX3w0ASNjVbWlpKj/JKGzY/U6nUFVfo6Pqj/4fUj4LAClZemWarXJWOKd2dXC43IyCEzwuFAoVCgXw+T6FQmLGMRCIx471KlXi+QwFV/KWlILBCzXZOORoI0uk0HR0drF+/nv7+fgqFAuPj4/T397N+/foZfQHhMsLXyWSSjo4Ouru72bZtG52dnWSzWXp6eujq6iKVSs0IAuVlUEVvHk2Ri3BwcNCHhoYaXYwVJTyGz+WK120lk8kZFTn6OpvNMjU1RTabBSCVSpFMJmlvbyeXy5HP58nn86XlhL3O4d4/XE8+nyebzdLS0kIqlZrRKThb52SUzgrUl5k1by5CqT0zK1XY8HX5+6GWlpYZzf1o0z6s0NH3ovNE1+XutLW1VVWRVfmXnoLACrbQimhms55HXsgytOde3jSegEjMKQiIxJyCgEjMKQiIxJyCgEjMKQiIxJyCgEjMKQiIxJyCgEjMKQiIxJyCgEjMKQiIxJyCgEjMKQiIxFy1acjWmtkTZvaymf3czD5gZuvM7GkzOxH87a5VYUWk9qptCfwF8L/c/b3A+ykmJn0AeMbdtwPPBK9FpEktOgiY2RrgNoK8Au6ecffzwH6KiUhBCUlFml41LYFtwCjw383ssJn9tZl1AL3uPhLMcxrorbaQIlI/1QSBFHAj8FV33wOMU9b09+LAdBVHMjWz+81syMyGRkdHqyiGiFSjmiAwDAy7+8Hg9RMUg8IZM9sIEPx9u9KH3f1hdx9098H169dXUQwRqUY1CUlPA2+a2c5g0u3AceApiolIQQlJRZpetaMN/2vgUTNLA68Bv0cxsHzTzO4D3gDuqXIdIlJHVQUBdz8CXJbMgGKrQESWAV0xKBJzCgIiMacgIBJzCgIiMdc0uQibITuyyHJQKbX7bLkgF1KvmiYIiMj83P2KdpjLKggoq63I3KIVulAozKgz1bSkmyYIiMjcoingKx0SLJY6BkWWoVq2nBUERGJOQUAk5hQERGJOQUAk5hQERGKuaU4R6opBkfqZq34pCIisUNE6tSyCQCKhIxORWlp2QUBEait6QdFcFxdp9ysScwoCIjFXbULSf2tmx8zsJTN7zMzazGybmR00s5Nm9nfBSMQi0qSqyUXYB/wbYNDddwNJ4F7gi8CX3f09wDngvloUVETqo9rDgRSwysxSQDswAvwmxWxEoISkIk2vmgxEbwH/GThFsfKPAYeA8+6eC2YbBvqqLaSI1E81hwPdFNOQbwM2AR3Aviv4vBKSijSBag4H/inwS3cfdfcs8C3gFmBtcHgAsBl4q9KHlZBUpDlUc7HQKeBmM2sHJimmHhsCfgR8HHicBSYkHRkZ4Qtf+EIVRRGJh3Cg0YWMLLTQS/EXHQTc/aCZPQE8D+SAw8DDwP8EHjezLwTTHplvWel0mq1bty62KFIm+uVHfyyFQgF3p1AokEwmL/shhT+u8nHswh9eoVCYsdxEIjHnUNeFQqE0IGY473w/3tnKLkX1CALWDDfuDA4O+tDQUKOLsWKElR2K92S4O/l8nunpafL5PPl8nvb2dpLJZKlyhp9JJBKl+ziiFTmXy5HJZIBi5WxpaSGdTpNKVd6P5PN5JicnyWQyM+aPrnO2z4Xr0P0ktWVmh9z9sgTCundgBQkrfljB8vk8Y2NjHDt2jO985zsMDQ0xMTGBmXH33Xeza9cu3vOe9/De9753RsXM5/Oliv/YY4/xwgsvcOjQISYmJhgYGGD37t3cdttt7Nixg76+vhkth1wux9mzZzl16hQ/+clP+MEPfsDFixdpbW3lQx/6ELt37+a6667j2muvJZVKkUwmZ/wP0QCUz+fnDBhSGwoCy1ylO8XCSpNIJEgmk2SzWUZHR/nVr37F5OQkLS0tXLhwgYmJCXK53Iy9btj8D5dx6dIlRkdHGR4eZmpqira2Nvr6+picnCxV0qjwc9lslrGxMd58803GxsZIp9O88847TE5Olso6V+UOyyH1pyCwAoSVpfx42sxKTfDp6Wmmp6fJ5XIzmvBhAIhW5rByJpPJ0t59amqKbDZLLpcjl8vNONYvL0symSx9dnp6momJidKhCEAqlSqVL9qKCP+G09QCWBoKAsvcXJ1tYZO6UCjM6BvIZDLkcrkZ/QbRz8y1ruh8YX/BXPOVV/bwebSjMNoBudDbX6V2FARWiGjlDSt+JpPh0qVLJBIJ+vv72b17NxMTEySTSfr6+uju7qa9vb30+ejeuFAoMD09zYYNG9i+fTtTU1NcvHiRzZs3s3nzZtauXUs6Xbw3LAwG+Xy+1FpIp9Ns2LCBXbt2cenSJVpbW+nr66Ojo4N8Pj/jbEM0pVYYDMLlpVIp9QvUmYLAChCtvOHePpvNlh6rVq1ix44dpNNppqamcHcGBgbo7e2lo6Oj9LnoHjlsMWzcuJFCoUBXVxfnz5+np6eHLVu2sG7dOlpbW2esM1xvIpGgvb2dzZs3s2fPnlIZrrnmGtatW1c6HICZZzKihwaFQqG0rIWcWpTF0ynCFaA8CGQymdLxdzqdrrgnTSQSM/bgUKyE4Sk8oGLHXzhfePYg2jeQTCZL/Q3RwFC+3GinX6VDEvh166LS9QyyODpFuIKVN6XDZjowo+KVCytm9DRdpeP32dYZ9jNUmjes+OGyy5czX+efWgBLR0FghQkr35XMP1dFX8znFvL5hZZN6k+XZInEnIKASMwpCIjEnIKASMwpCIjEnIKASMwpCIjEnIKASMwpCIjEnIKASMwpCIjE3LxBwMy+ZmZvm9lLkWnrzOxpMzsR/O0OppuZ/WWQjPRFM7uxnoUXkeotpCXwdS7PLPQA8Iy7bweeCV4D3AlsDx73A1+tTTFFpF7mDQLu/mPg3bLJ+ykmG4WZSUf3A3/jRQcoZiPaWKvCikjtLbZPoNfdR4Lnp4He4Hkf8GZkPiUkFWlyVXcMenFUiSsenkgJSUWaw2KDwJmwmR/8fTuY/hawJTKfEpKKNLnFBoGnKCYbhZlJR58Cfic4S3AzMBY5bBCRJjTv8GJm9hjwQeAqMxsG/iPwZ8A3zew+4A3gnmD27wF3ASeBCeD36lBmEamheYOAu39ylrdurzCvA5+ptlAisnR0xaBIzCkIiMScgoBIzCkIiMRcUyQfcXempqYaXQyRWGqKIBBmwBWRpdcUQSCZTLJ69epGF0PK1CJZrVKJNb+mCAKgH8tilWf3LZ8+3+fCnILlKcKjuQbDbMdhBuIwGWn4fjSrcZiktNIjzDBsZqXPRD8X/Q2En6mUUVlqq2mCgNTO1NQUmUyGiYkJxsfHS5W3q6uL9vZ2Ojo6SinDwwqYzWbJZDKMj48zNTVVer+trY10Ok1raystLS0zMg2HCoUCUDkI5PN5crkc09PTTE5Oks/nS+tqb29n1apVrF69mpaWllKFDz8rS0NBYJmrlOL7lVde4eWXX+bZZ5/l+9//PmNjY7S1tXHPPfdwxx13cPfdd+Pupc/kcjnOnz/P8PAw3/72t3nuuecYGRlhYmKCW2+9lV27dnHTTTdx880309HRUfpcoVCgUCiU9uqJRGJGawFgenqac+fO8frrr/Pkk0/y4osvcvz4cS5dusRHPvIRPvjBD/Lxj3+czs5OWltbS+UKsyurFVB/CgIrXLh3TiSKZ4PDShvudcNHV1cXW7du5d577+WOO+5gamqK6elpXnrpJU6fPs3Xv/51HnroIQYGBrjuuuvYu3cvfX199PT0kEoVf0buTjab5Y033mB4eJijR4/y4osvMjU1RSqVYseOHXziE59g/fr1JBIJNmzYwPr161m9ejWpVOqyQw0FgKWhILBCRStUMpksNeXD5nr5vGFzv7u7e8aePjwuP3v2LKOjo4yMjNDa2ko6nWZkZIR169axZs2aUjP//PnznDlzhrNnz3Lq1Cnefbc4KNXq1asZGBhg165d7Nixg9bW1tL6o62S8r4IBYL6UxBY5iodO4d7/rDit7S00NHRQXt7O21tbaRSKcbHx0t73HQ6XdqbZzKZUkuhtbWVu+66izvvvBOAVCrFkSNHePbZZ3n00Ud59dVXGRsbY9++fUxMTHD27FkOHDjATTfdxPXXX8+HP/xhHnzwQTo6Oshms6RSKVKpVCkY5XI5stksuVyu1NeQSqVKfQK5XI5UKqXOwTqzZuiAGRwc9KGhoUYXY1mKfn/h84mJCaamprh48SLnzp2jUCiQTCbp7u5m9erVrFmzptSZB8yoZOVnCMrXMT4+ztjYGOfPny91IK5du5ZCoUAul+PSpUt0dnayatUq1qxZw5o1a0oBJlqRw1ZG+VmA6DoLhULpMEGqZ2aH3H2wfLpaAk0mWgmv9DPh8/b2dtrb2+np6WHr1q0V5w37CMrXE04P34s2yd2djo4OOjo62Lhx5vixs5W3vOOy/P+r1LEZPi8/CyH1oSCwQoUVKVqpF6JSJa0UmK604y7a8igvV7RFsNhyy+IpCDSZK2361roXvdKyarH8uSq1zgQ0lsKtSMwpCIjEnIKASMwtNiHpn5vZy0HS0SfNbG3kvc8HCUlfMbM76lVwEamNxSYkfRrY7e43AL8APg9gZruAe4Hrgs98xcx0nkekiS0qIam7/9Ddc8HLAxQzDUExIenj7j7t7r+kmH9gbw3LKyI1Vos+gd8Hvh88V0JSkWWmqiBgZg8COeDRRXxWCUlFmsCig4CZ/S7wUeBT/uvrVpWQVGSZWdQVg2a2D/gj4J+4+0TkraeAvzWzLwGbgO3A/6u6lDFSy1F15roPYa4r9CrdOFTp9XzKrwSsdLNT+Tp15eDSW2xC0s8DrcDTwZd2wN3/pbsfM7NvAscpHiZ8xt3zlZcstXAlYwkutBKH8xUKhctuNCoPUpXGIZxtRKDonYOVbhiajQJDfS02Iekjc8z/p8CfVlOoOFvIdfRz7aHD23LL34uO3Rfeohu+H6244ZBh4SO8/z86IGk+ny8FiHAYsHw+TyaTYWpqivb29tKYgdHhwsKxA/L5PG1tbZcNJFJ+f4EGFVkauoFoBQrH/Iveqw/w05/+lOeff54DBw7w9ttv09nZybp169izZw833XQTAwMD9PT0XDa4R1jBs9ls6b2WlpbSnr1QKHD27FnOnDnDkSNHOHjwIGfOnGFycpKtW7dy4403cvfdd7NhwwZaW1tLAQZ+HQDCcQNma2lI/SgILFOV7vWf7b2wooUDffT29pJOp0uV8cSJE0xNTfHyyy+zadMmNmzYQHd3Nxs3brxsfIFwmRcvXuSdd97h3Xff5cyZM5w7d46xsTHOnj1LIpGgp6cHgN7e3hk5JSoNYDLX/yj1p5GFmthc3015h1v5KD3R98LKGw7/ncvlaG1t5eTJkxw/fpwnn3ySo0ePcu7cOfr7+9m7dy/vf//72bdvH11dXaUhvsK9dz6f55VXXuHQoUMcPnyYZ555BiiOI7h3717279/Pjh072LRpU6mlkM/nS8sJlxXd64fvlf9Pus24dmYbWUhBIAYqnXEIg0Eul2NycpKJiQkuXrxYCgyvvfYahw8f5n3vex+9vb309vYyPT3N8PAwv/zlL0kmk+zcuZPt27ezZ88eenp6aG9vJ5lM0tHRQUtLC62traX+hIUe35d3YCoA1I6GF4uxShUpHEi0ra2Nrq4u8vk8U1NTtLW10dnZyZYtW7j66qvZtGlTaVzCXC7Hxo0b2bp1K+l0mv7+fjZv3szOnTvp7Ows9RNEhzKPPqopr9SPgkBMlJ/Ki57GC193dHSwfft2duzYcUXLju65lTBk+VEQiJlKe+VqK60q/fKmQUVEYk5BQCTmFAREYk5BQCTmFAREYk5BQCTmFAREYk5BQCTmFAREYk5BQCTmFAREYk5BQCTmFAREYm5RCUkj7/2hmbmZXRW8NjP7yyAh6YtmdmM9Ci0itbPYhKSY2RbgI8CpyOQ7KeYa2A7cD3y1+iKKSD0tKiFp4MsUE5BEx63aD/yNFx0A1prZxpqUVETqYlF9Ama2H3jL3V8oe0sJSUWWmSseWcjM2oE/pngosGhmdj/FQwb6+/urWZSIVGExLYFrgW3AC2b2OsWko8+b2dUoIanIsnPFQcDdj7r7BncfcPcBik3+G939NMWEpL8TnCW4GRhz95HaFllEamkhpwgfA34K7DSzYTO7b47Zvwe8BpwE/hvwr2pSShGpm8UmJI2+PxB57sBnqi+WiCwVXTEoEnMKAiIxpyAgEnMKAiIxpyAgEnMKAiIxpyAgEnMKAiIxpyAgEnNWvMivwYUwGwXGgbONLkvEVag882m2Mqk8c9vq7pfdrdcUQQDAzIbcfbDR5QipPPNrtjKpPIujwwGRmFMQEIm5ZgoCDze6AGVUnvk1W5lUnkVomj4BEWmMZmoJiEgDNDwImNk+M3slSFjyQIPKsMXMfmRmx83smJl9Npj+J2b2lpkdCR53LWGZXjezo8F6h4Jp68zsaTM7EfztXqKy7IxsgyNmdsHMPrfU26dSIpzZtslSJMKZpTx/bmYvB+t80szWBtMHzGwysq0eqnV5Fs3dG/YAksCrwDVAGngB2NWAcmykOE4iQBfwC2AX8CfAv2/QtnkduKps2n8CHgiePwB8sUHf2Wlg61JvH+A24Ebgpfm2CXAX8H3AgJuBg0tUno8AqeD5FyPlGYjO10yPRrcE9gIn3f01d88Aj1NMYLKk3H3E3Z8Pnl8Efk5z5kvYD3wjeP4N4LcbUIbbgVfd/Y2lXrFXToQz2zapeyKcSuVx9x+6ey54eYDiiNtNrdFBoOmSlZjZALAHOBhM+oOgafe1pWp+Bxz4oZkdCnI0APT6r0dvPg30LmF5QvcCj0VeN2r7hGbbJs3w2/p9iq2R0DYzO2xmz5nZrUtcllk1Ogg0FTPrBP4B+Jy7X6CYS/Fa4B8BI8B/WcLi/Ia730gxv+NnzOy26JtebGMu6akdM0sDHwP+PpjUyO1zmUZsk9mY2YNADng0mDQC9Lv7HuDfAX9rZqsbVb6oRgeBBScrqTcza6EYAB51928BuPsZd8+7e4HiEOp7l6o87v5W8Pdt4Mlg3WfCJm3w9+2lKk/gTuB5dz8TlK1h2yditm3SsN+Wmf0u8FHgU0Fgwt2n3f2d4Pkhin1hO5aiPPNpdBD4GbDdzLYFe5l7KSYwWVJmZsAjwM/d/UuR6dFjyH8GXJaevU7l6TCzrvA5xc6mlyhum08Hs30a+PZSlCfik0QOBRq1fcrMtk0akgjHzPZRTNT7MXefiExfb2bJ4Pk1FDN3v1bv8ixIo3smKfbi/oJiZHywQWX4DYrNyBeBI8HjLuB/AEeD6U8BG5eoPNdQPFPyAnAs3C5AD/AMcAL438C6JdxGHcA7wJrItCXdPhQD0AiQpXiMf99s24TiWYG/Cn5XR4HBJSrPSYp9EeHv6KFg3n8efJdHgOeB32rEb73SQ1cMisRcow8HRKTBFAREYk5BQCTmFAREYk5BQCTmFAREYk5BQCTmFAREYu7/A0QYB+0ro7EiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2g04ZHZtp3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "534781f2-97f4-4b20-b480-5604deee617a"
      },
      "source": [
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  \n",
        "\n",
        "        path = os.path.join(DATADIR,category)  # create path to the labels\n",
        "        class_num = CATEGORIES.index(category)  # get the classification \n",
        "        # print(class_num)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per label\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "            #except OSError as e:\n",
        "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "            #except Exception as e:\n",
        "            #    print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 613/613 [00:02<00:00, 241.75it/s]\n",
            "100%|██████████| 299/299 [00:01<00:00, 154.81it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 328.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnoDAW3tvAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(training_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwO-RlltvJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for sample in training_data[:10]:\n",
        "#     print(sample[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyb7sMsHty6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "\n",
        "# print(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fk4CZJ4t2EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle_out = open(\"X.pickle\",\"wb\")\n",
        "pickle.dump(X, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"y.pickle\",\"wb\")\n",
        "pickle.dump(y, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wENpa8mit8dR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "1328851c-0fd2-493c-8f79-c7e9385fc952"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.29.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE6-oEAdt-3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "94e1c539-c834-4bd5-b854-cdd961ab5242"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "import pickle\n",
        "\n",
        "X = X/255.0\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "\n",
        "model.add(Dense(64))\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 512)     5120      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 148, 148, 512)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 72, 72, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 34, 34, 256)       1179904   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 34, 34, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 73984)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4735040   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 195       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 8,280,067\n",
            "Trainable params: 8,280,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLnAH1UE5LFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "796b3c30-2101-49e7-f405-25836d043c85"
      },
      "source": [
        "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 788 samples, validate on 198 samples\n",
            "Epoch 1/10\n",
            "788/788 [==============================] - 1275s 2s/sample - loss: 2.3068 - acc: 0.6954 - val_loss: 0.2233 - val_acc: 0.9293\n",
            "Epoch 2/10\n",
            "788/788 [==============================] - 1287s 2s/sample - loss: 0.3550 - acc: 0.8731 - val_loss: 0.1973 - val_acc: 0.9343\n",
            "Epoch 3/10\n",
            "788/788 [==============================] - 1308s 2s/sample - loss: 0.2123 - acc: 0.9226 - val_loss: 0.0919 - val_acc: 0.9697\n",
            "Epoch 4/10\n",
            "788/788 [==============================] - 1308s 2s/sample - loss: 0.1706 - acc: 0.9454 - val_loss: 0.1182 - val_acc: 0.9495\n",
            "Epoch 5/10\n",
            "788/788 [==============================] - 1263s 2s/sample - loss: 0.1333 - acc: 0.9619 - val_loss: 0.1078 - val_acc: 0.9596\n",
            "Epoch 6/10\n",
            "788/788 [==============================] - 1273s 2s/sample - loss: 0.1040 - acc: 0.9683 - val_loss: 0.2490 - val_acc: 0.9242\n",
            "Epoch 7/10\n",
            "788/788 [==============================] - 1247s 2s/sample - loss: 0.0965 - acc: 0.9721 - val_loss: 0.0634 - val_acc: 0.9798\n",
            "Epoch 8/10\n",
            "788/788 [==============================] - 1254s 2s/sample - loss: 0.0572 - acc: 0.9835 - val_loss: 0.0923 - val_acc: 0.9545\n",
            "Epoch 9/10\n",
            "788/788 [==============================] - 1251s 2s/sample - loss: 0.0546 - acc: 0.9860 - val_loss: 0.0831 - val_acc: 0.9697\n",
            "Epoch 10/10\n",
            "788/788 [==============================] - 1248s 2s/sample - loss: 0.0401 - acc: 0.9873 - val_loss: 0.0582 - val_acc: 0.9798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0fbd106d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMKCHqm1sfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('column-classification-CNN.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uscSyI-ckcDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "CATEGORIES = [\"Amount\", \"Captions\", \"Sr.No\"]\n",
        "\n",
        "\n",
        "def prepare(img1_array):\n",
        "    IMG_SIZE = 150  \n",
        "    # img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    # plt.imshow(img_array, cmap='gray')  # graph it\n",
        "    # plt.show()\n",
        "    new_array = cv2.resize(img1_array, (IMG_SIZE, IMG_SIZE))\n",
        "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "\n",
        "# count=10\n",
        "for category in CATEGORIES:  # do dogs and cats\n",
        "    path = os.path.join(DATADIR,category)  # create path \n",
        "    count=10\n",
        "    for img in os.listdir(path):  # iterate over each image per label\n",
        "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "        plt.imshow(img_array, cmap='gray')  # graph it\n",
        "        plt.show()  # display!\n",
        "        prediction = model.predict([prepare(img_array)])\n",
        "        print(prediction)\n",
        "        for i in range(len(prediction[0])):\n",
        "          if prediction[0][i] == 1:\n",
        "            print(CATEGORIES[int(list(prediction[0]).index(1))])\n",
        "        count-=1\n",
        "        if count==0:\n",
        "          break \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV_aVBgTxfBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for category in CATEGORIES:  \n",
        "#     path = os.path.join(DATADIR,category)  \n",
        "#     for img in os.listdir(path):  # iterate over each image\n",
        "#         img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE) # convert to array\n",
        "#         new_array = cv2.resize(img_array, (200,150))  \n",
        "#         # plt.imshow(img_array, cmap='gray')  # graph it\n",
        "#         plt.imshow(new_array, cmap='gray')  # graph it\n",
        "#         plt.show()  # display!\n",
        "\n",
        "#         break  # we just want one for now so break\n",
        "# #     break  #...and one more!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1zcRPkZxOFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMG_SIZE = 50\n",
        "\n",
        "# new_array = cv2.resize(img_array, (200,150))\n",
        "# plt.imshow(new_array, cmap='gray')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZRkgxPL0iux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMG_SIZE = 150\n",
        "# new_array = cv2.resize(img_array, (200, 150))\n",
        "# training_data = []\n",
        "\n",
        "# def create_training_data():\n",
        "#     for category in CATEGORIES:  # do dogs and cats\n",
        "\n",
        "#         path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
        "#         class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
        "#         print(class_num)\n",
        "\n",
        "#         for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "#             try:\n",
        "#                 img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "#                 new_array = cv2.resize(img_array, (200, 150))  # resize to normalize data size\n",
        "#                 training_data.append([new_array, class_num])  # add this to our training_data\n",
        "#             except Exception as e:  # in the interest in keeping the output clean...\n",
        "#                 pass\n",
        "#             #except OSError as e:\n",
        "#             #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "#             #except Exception as e:\n",
        "#             #    print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "# create_training_data()\n",
        "\n",
        "# print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG64V-uG0osY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import random\n",
        "# random.shuffle(training_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6TcoPUC0wD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = []\n",
        "# y = []\n",
        "\n",
        "# for features,label in training_data:\n",
        "#     X.append(features)\n",
        "#     y.append(label)\n",
        "\n",
        "# print(X[0].reshape(-1, 200, 150, 1))\n",
        "\n",
        "# X = np.array(X).reshape(-1, 200, 150, 1)\n",
        "# print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WisCFMXX1JLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "d7359ef3-985f-409a-bc4c-fbc822e5aa1f"
      },
      "source": [
        "X = X/255.0\n",
        "\n",
        "model_1 = Sequential()\n",
        "\n",
        "model_1.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_1.add(Conv2D(256, (3, 3)))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_1.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "\n",
        "model_1.add(Dense(64))\n",
        "\n",
        "model_1.add(Dense(3))\n",
        "model_1.add(Activation('softmax'))\n",
        "\n",
        "model_1.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_1.fit(X, y, batch_size=32, epochs=10, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 690 samples, validate on 296 samples\n",
            "Epoch 1/10\n",
            "690/690 [==============================] - 400s 580ms/sample - loss: 4.5820 - acc: 0.5507 - val_loss: 0.7920 - val_acc: 0.6385\n",
            "Epoch 2/10\n",
            "690/690 [==============================] - 397s 576ms/sample - loss: 0.6918 - acc: 0.7188 - val_loss: 0.5927 - val_acc: 0.7939\n",
            "Epoch 3/10\n",
            "690/690 [==============================] - 397s 575ms/sample - loss: 0.5791 - acc: 0.7696 - val_loss: 0.5494 - val_acc: 0.7804\n",
            "Epoch 4/10\n",
            "690/690 [==============================] - 396s 574ms/sample - loss: 0.5235 - acc: 0.7768 - val_loss: 0.5715 - val_acc: 0.7770\n",
            "Epoch 5/10\n",
            "690/690 [==============================] - 396s 574ms/sample - loss: 0.4446 - acc: 0.8145 - val_loss: 0.5580 - val_acc: 0.7770\n",
            "Epoch 6/10\n",
            "690/690 [==============================] - 396s 573ms/sample - loss: 0.4003 - acc: 0.8203 - val_loss: 0.5189 - val_acc: 0.7838\n",
            "Epoch 7/10\n",
            "690/690 [==============================] - 397s 575ms/sample - loss: 0.3545 - acc: 0.8464 - val_loss: 0.6364 - val_acc: 0.7432\n",
            "Epoch 8/10\n",
            "690/690 [==============================] - 396s 573ms/sample - loss: 0.3335 - acc: 0.8594 - val_loss: 0.5584 - val_acc: 0.7770\n",
            "Epoch 9/10\n",
            " 96/690 [===>..........................] - ETA: 5:04 - loss: 0.2518 - acc: 0.8854"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-55fcc04942e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}