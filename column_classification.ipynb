{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "column_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yogita98/tensorflow-model-deployment/blob/master/column_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyM94owUpNz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7b51457f-f543-4e26-d349-f0bde10bc59d"
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTxU9jN-yisA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cat /var/log/colab-jupyter.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dbf4GGLpbgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RWSJpsyKqHjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf96a1db-e008-4635-b526-b5e0e146088b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7cBHyzGpbpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATADIR = \"\"\n",
        "\n",
        "CATEGORIES = [\"label1\", \"label2\", \"labeln\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBrXj7jwpbzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for category in CATEGORIES:  \n",
        "    path = os.path.join(DATADIR,category)  \n",
        "    for img in os.listdir(path):  # iterate over each image per label\n",
        "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "        plt.imshow(img_array, cmap='gray')  # graph it\n",
        "        plt.show()  # display!\n",
        "\n",
        "        break  # we just want one for now so break\n",
        "     break  #...and one more!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC5O0bRDpmk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXp5cLTXtiqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(img_array.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS8J7Qikti0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 150\n",
        "\n",
        "new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n",
        "plt.imshow(new_array, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2g04ZHZtp3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "534781f2-97f4-4b20-b480-5604deee617a"
      },
      "source": [
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  \n",
        "\n",
        "        path = os.path.join(DATADIR,category)  # create path to the labels\n",
        "        class_num = CATEGORIES.index(category)  # get the classification \n",
        "        # print(class_num)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per label\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "            #except OSError as e:\n",
        "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "            #except Exception as e:\n",
        "            #    print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 613/613 [00:02<00:00, 241.75it/s]\n",
            "100%|██████████| 299/299 [00:01<00:00, 154.81it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 328.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnoDAW3tvAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(training_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwO-RlltvJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for sample in training_data[:10]:\n",
        "#     print(sample[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyb7sMsHty6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "\n",
        "# print(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fk4CZJ4t2EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle_out = open(\"X.pickle\",\"wb\")\n",
        "pickle.dump(X, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"y.pickle\",\"wb\")\n",
        "pickle.dump(y, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wENpa8mit8dR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "1328851c-0fd2-493c-8f79-c7e9385fc952"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.29.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE6-oEAdt-3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "94e1c539-c834-4bd5-b854-cdd961ab5242"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "import pickle\n",
        "\n",
        "X = X/255.0\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "\n",
        "model.add(Dense(64))\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 512)     5120      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 148, 148, 512)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 72, 72, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 34, 34, 256)       1179904   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 34, 34, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 73984)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4735040   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 195       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 8,280,067\n",
            "Trainable params: 8,280,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLnAH1UE5LFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "796b3c30-2101-49e7-f405-25836d043c85"
      },
      "source": [
        "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 788 samples, validate on 198 samples\n",
            "Epoch 1/10\n",
            "788/788 [==============================] - 1275s 2s/sample - loss: 2.3068 - acc: 0.6954 - val_loss: 0.2233 - val_acc: 0.9293\n",
            "Epoch 2/10\n",
            "788/788 [==============================] - 1287s 2s/sample - loss: 0.3550 - acc: 0.8731 - val_loss: 0.1973 - val_acc: 0.9343\n",
            "Epoch 3/10\n",
            "788/788 [==============================] - 1308s 2s/sample - loss: 0.2123 - acc: 0.9226 - val_loss: 0.0919 - val_acc: 0.9697\n",
            "Epoch 4/10\n",
            "788/788 [==============================] - 1308s 2s/sample - loss: 0.1706 - acc: 0.9454 - val_loss: 0.1182 - val_acc: 0.9495\n",
            "Epoch 5/10\n",
            "788/788 [==============================] - 1263s 2s/sample - loss: 0.1333 - acc: 0.9619 - val_loss: 0.1078 - val_acc: 0.9596\n",
            "Epoch 6/10\n",
            "788/788 [==============================] - 1273s 2s/sample - loss: 0.1040 - acc: 0.9683 - val_loss: 0.2490 - val_acc: 0.9242\n",
            "Epoch 7/10\n",
            "788/788 [==============================] - 1247s 2s/sample - loss: 0.0965 - acc: 0.9721 - val_loss: 0.0634 - val_acc: 0.9798\n",
            "Epoch 8/10\n",
            "788/788 [==============================] - 1254s 2s/sample - loss: 0.0572 - acc: 0.9835 - val_loss: 0.0923 - val_acc: 0.9545\n",
            "Epoch 9/10\n",
            "788/788 [==============================] - 1251s 2s/sample - loss: 0.0546 - acc: 0.9860 - val_loss: 0.0831 - val_acc: 0.9697\n",
            "Epoch 10/10\n",
            "788/788 [==============================] - 1248s 2s/sample - loss: 0.0401 - acc: 0.9873 - val_loss: 0.0582 - val_acc: 0.9798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0fbd106d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMKCHqm1sfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('column-classification-CNN.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uscSyI-ckcDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "CATEGORIES = [\"label1\", \"label2\", \"labeln\"]\n",
        "\n",
        "def prepare(img1_array):\n",
        "    IMG_SIZE = 150  \n",
        "    # img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    # plt.imshow(img_array, cmap='gray')  # graph it\n",
        "    # plt.show()\n",
        "    new_array = cv2.resize(img1_array, (IMG_SIZE, IMG_SIZE))\n",
        "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "\n",
        "# count=10\n",
        "for category in CATEGORIES:  # do dogs and cats\n",
        "    path = os.path.join(DATADIR,category)  # create path \n",
        "    count=10\n",
        "    for img in os.listdir(path):  # iterate over each image per label\n",
        "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "        plt.imshow(img_array, cmap='gray')  # graph it\n",
        "        plt.show()  # display!\n",
        "        prediction = model.predict([prepare(img_array)])\n",
        "        print(prediction)\n",
        "        for i in range(len(prediction[0])):\n",
        "          if prediction[0][i] == 1:\n",
        "            print(CATEGORIES[int(list(prediction[0]).index(1))])\n",
        "        count-=1\n",
        "        if count==0:\n",
        "          break \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV_aVBgTxfBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for category in CATEGORIES:  \n",
        "#     path = os.path.join(DATADIR,category)  \n",
        "#     for img in os.listdir(path):  # iterate over each image\n",
        "#         img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE) # convert to array\n",
        "#         new_array = cv2.resize(img_array, (200,150))  \n",
        "#         # plt.imshow(img_array, cmap='gray')  # graph it\n",
        "#         plt.imshow(new_array, cmap='gray')  # graph it\n",
        "#         plt.show()  # display!\n",
        "\n",
        "#         break  # we just want one for now so break\n",
        "# #     break  #...and one more!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1zcRPkZxOFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMG_SIZE = 50\n",
        "\n",
        "# new_array = cv2.resize(img_array, (200,150))\n",
        "# plt.imshow(new_array, cmap='gray')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZRkgxPL0iux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMG_SIZE = 150\n",
        "# new_array = cv2.resize(img_array, (200, 150))\n",
        "# training_data = []\n",
        "\n",
        "# def create_training_data():\n",
        "#     for category in CATEGORIES:  # do dogs and cats\n",
        "\n",
        "#         path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
        "#         class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
        "#         print(class_num)\n",
        "\n",
        "#         for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "#             try:\n",
        "#                 img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "#                 new_array = cv2.resize(img_array, (200, 150))  # resize to normalize data size\n",
        "#                 training_data.append([new_array, class_num])  # add this to our training_data\n",
        "#             except Exception as e:  # in the interest in keeping the output clean...\n",
        "#                 pass\n",
        "#             #except OSError as e:\n",
        "#             #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "#             #except Exception as e:\n",
        "#             #    print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "# create_training_data()\n",
        "\n",
        "# print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG64V-uG0osY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import random\n",
        "# random.shuffle(training_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6TcoPUC0wD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = []\n",
        "# y = []\n",
        "\n",
        "# for features,label in training_data:\n",
        "#     X.append(features)\n",
        "#     y.append(label)\n",
        "\n",
        "# print(X[0].reshape(-1, 200, 150, 1))\n",
        "\n",
        "# X = np.array(X).reshape(-1, 200, 150, 1)\n",
        "# print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WisCFMXX1JLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = X/255.0\n",
        "\n",
        "# model_1 = Sequential()\n",
        "\n",
        "# model_1.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "# model_1.add(Activation('relu'))\n",
        "# model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# model_1.add(Conv2D(256, (3, 3)))\n",
        "# model_1.add(Activation('relu'))\n",
        "# model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# model_1.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "\n",
        "# model_1.add(Dense(64))\n",
        "\n",
        "# model_1.add(Dense(3))\n",
        "# model_1.add(Activation('softmax'))\n",
        "\n",
        "# model_1.compile(loss='sparse_categorical_crossentropy',\n",
        "#               optimizer='adam',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model_1.fit(X, y, batch_size=32, epochs=10, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}